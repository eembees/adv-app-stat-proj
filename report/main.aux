\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sterling_1001_2013}
\citation{hastie_elements_2017}
\citation{efron_least_2004}
\citation{courtney_comments_2008}
\citation{paisley_columbiax:_2018}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{LASSO or $L_1$ regularization in linear models}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}The LASSO}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}General problem}{1}{section*.5}}
\citation{tibshirani_regression_1996}
\citation{pedregosa_scikit-learn:_2011}
\citation{tibshirani_regression_1996}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}The LASSO objective}{2}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Consequences of using a LASSO penalty}{2}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Logistic Regression}{2}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}General form}{2}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the principle of LASSO. $w_{LS}$ is the least squares estimate of w. The red lines are the contours of $\delimiter 69640972 \delimiter 69640972 y - Xw \delimiter 86418188 \delimiter 86418188 _2^2$, similar to a likelihood contour. The blue lines are the contours of $\delimiter 69640972 \delimiter 69640972 w \delimiter 86418188 \delimiter 86418188 _1$, at two different values of $\lambda $ (one is labelled in green). Two red dots are plotted, showing vector solutions $w$ for each of the two $\lambda $ values. That is, they are minima of $\delimiter 69640972 \delimiter 69640972 y - Xw \delimiter 86418188 \delimiter 86418188 ^2_2 + \lambda \delimiter 69640972 \delimiter 69640972 w \delimiter 86418188 \delimiter 86418188 _1$ for two different $\lambda $s. As can be seen, for the lower value of $\lambda $, $w_2 = 0$, as was part of the objective. }}{2}{figure.1}}
\newlabel{LASSOexplained}{{1}{2}{Illustration of the principle of LASSO. $w_{LS}$ is the least squares estimate of w. The red lines are the contours of $\lvert \lvert y - Xw \rvert \rvert _2^2$, similar to a likelihood contour. The blue lines are the contours of $\lvert \lvert w \rvert \rvert _1$, at two different values of $\lambda $ (one is labelled in green). Two red dots are plotted, showing vector solutions $w$ for each of the two $\lambda $ values. That is, they are minima of $\lvert \lvert y - Xw \rvert \rvert ^2_2 + \lambda \lvert \lvert w \rvert \rvert _1$ for two different $\lambda $s. As can be seen, for the lower value of $\lambda $, $w_2 = 0$, as was part of the objective}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Profiles of LASSO coefficients as the tuning parameter is increased. Lines are drawn at the parameter value at which a new nonzero coefficient is included. As can be seen, two out of the three largest values at $\lambda = 1$ are already singled out before $\lambda = 0.1$. When the shrinkage factor s = 1, $w_{lasso} = w_{ls}$, the least squares estimate, while if s = 0, all coefficients are zero, as the penalty on $w$ is infinity. }}{2}{figure.2}}
\newlabel{LASSOpath}{{2}{2}{Profiles of LASSO coefficients as the tuning parameter is increased. Lines are drawn at the parameter value at which a new nonzero coefficient is included. As can be seen, two out of the three largest values at $\lambda = 1$ are already singled out before $\lambda = 0.1$. When the shrinkage factor s = 1, $w_{lasso} = w_{ls}$, the least squares estimate, while if s = 0, all coefficients are zero, as the penalty on $w$ is infinity}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Illustration of the principle of a hyperplane in 2 dimensions (in this case, a hyper-line). x signifies a vector $x = [ 1, x_1, x_2 ]^T$, while $w = [ w_0, w_1, w_2 ]^T$. The hyperplane $H$ is perpendicular to $w$, and classifies $x$. As $x$ lies in the positive direction of $H$ (that is, the direction of $w$) $y=1$ for this particular x. The offset of the hyperplane ($w_0$) is also signified. }}{3}{figure.3}}
\newlabel{hyperplane}{{3}{3}{Illustration of the principle of a hyperplane in 2 dimensions (in this case, a hyper-line). x signifies a vector $x = [ 1, x_1, x_2 ]^T$, while $w = [ w_0, w_1, w_2 ]^T$. The hyperplane $H$ is perpendicular to $w$, and classifies $x$. As $x$ lies in the positive direction of $H$ (that is, the direction of $w$) $y=1$ for this particular x. The offset of the hyperplane ($w_0$) is also signified}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Logistic link (sigmoid) function}{3}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sigmoid function of link function ($x_{i}^T\cdot w$). As the link function moves further away from 0, the probability increases or decreases accordingly. }}{3}{figure.4}}
\newlabel{sigmoid}{{4}{3}{Sigmoid function of link function ($x_{i}^T\cdot w$). As the link function moves further away from 0, the probability increases or decreases accordingly}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Finding the optimum value of w}{3}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Regularization}{3}{section*.12}}
\citation{hastie_elements_2017}
\citation{noauthor_karakterfordeling_nodate}
\citation{noauthor_selenium_nodate}
\citation{noauthor_scrapy_nodate}
\citation{mckinney_data_2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}$L_1$ regularized Logistic Regression on South African Heart Disease dataset}{4}{section*.13}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Copenhagen University grade statistics}{4}{section*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Scraping and retrieving data}{4}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Data cleaning and removal}{4}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces L1 Regularized Logistic Regression on South African Heart Disease data \textbf  {Top}: Coefficient paths, as in figure \ref  {LASSOpath}, for the best performing cross validation fold. I used 5 random folds in total. \textbf  {Bottom}: Classification rates for the 5 folds. Interestingly, the maximum rate of classification occurs while the coefficients for alcohol use, obesity, and adiposity are still zero, suggesting that these variables are sensistive to overfitting. }}{4}{figure.5}}
\newlabel{SAHeart}{{5}{4}{L1 Regularized Logistic Regression on South African Heart Disease data \textbf {Top}: Coefficient paths, as in figure \ref {LASSOpath}, for the best performing cross validation fold. I used 5 random folds in total. \textbf {Bottom}: Classification rates for the 5 folds. Interestingly, the maximum rate of classification occurs while the coefficients for alcohol use, obesity, and adiposity are still zero, suggesting that these variables are sensistive to overfitting}{figure.5}{}}
\bibdata{mainNotes,adv_app_stat}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Initial view of the data}{5}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Bar plots of the four distributions of grades. Values are calculated as the mean ratio of each grade, with the error as the standard deviation on the mean. NS means "No show". As is evident, SCIENCE does differ slightly from the other faculties, with signifcantly more 00 grades.}}{5}{figure.6}}
\newlabel{overlayKU}{{6}{5}{Bar plots of the four distributions of grades. Values are calculated as the mean ratio of each grade, with the error as the standard deviation on the mean. NS means "No show". As is evident, SCIENCE does differ slightly from the other faculties, with signifcantly more 00 grades}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Classifying SCIENCE vs not-SCIENCE}{5}{section*.18}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Concluding remarks}{5}{section*.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces L1 Regularized Logistic Regression on KU grade data \textbf  {Top}: Coefficient paths, as in figure \ref  {LASSOpath}, for the best performing cross validation fold. I used 5 random folds in total. \textbf  {Bottom}: Classification rates for the 5 folds. The 2nd fold seems to outperform the others, for some reason. Again we see that some constraint on the weight vector yields a better classification rate. Interestingly, I did not get significantly higher rates of classification after either filtering the data by the nubmer of students, or by feature engineering more variables, as can be seen in the Appendix. }}{5}{figure.7}}
\newlabel{}{{7}{5}{L1 Regularized Logistic Regression on KU grade data \textbf {Top}: Coefficient paths, as in figure \ref {LASSOpath}, for the best performing cross validation fold. I used 5 random folds in total. \textbf {Bottom}: Classification rates for the 5 folds. The 2nd fold seems to outperform the others, for some reason. Again we see that some constraint on the weight vector yields a better classification rate. Interestingly, I did not get significantly higher rates of classification after either filtering the data by the nubmer of students, or by feature engineering more variables, as can be seen in the Appendix}{figure.7}{}}
\bibcite{noauthor_karakterfordeling_nodate}{{1}{}{{noa}}{{}}}
\bibcite{noauthor_scrapy_nodate}{{2}{}{{noa}}{{}}}
\bibcite{noauthor_selenium_nodate}{{3}{}{{noa}}{{}}}
\bibcite{courtney_comments_2008}{{4}{2008}{{Courtney and Courtney}}{{}}}
\bibcite{efron_least_2004}{{5}{2004}{{Efron et~al.}}{{Efron, Hastie, Johnstone, and Tibshirani}}}
\bibcite{hastie_elements_2017}{{6}{2017}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{mckinney_data_2010}{{7}{2010}{{McKinney}}{{}}}
\bibcite{paisley_columbiax:_2018}{{8}{2018}{{Paisley}}{{}}}
\bibcite{pedregosa_scikit-learn:_2011}{{9}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{sterling_1001_2013}{{10}{2013}{{Sterling}}{{}}}
\bibcite{tibshirani_regression_1996}{{11}{1996}{{Tibshirani}}{{}}}
\bibstyle{abbrvnat}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{6}{section*.20}}
\newlabel{LastBibItem}{{11}{6}{}{section*.20}{}}
\newlabel{LastPage}{{}{6}{}{}{}}
