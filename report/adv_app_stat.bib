
@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	note = {2018-06-02},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	pages = {179--188},
	file = {Full Text PDF:/Users/mag/Zotero/storage/YJEXGXNP/Fisher - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:/Users/mag/Zotero/storage/8FR542XD/j.1469-1809.1936.tb02137.html:text/html}
}

@book{barlow_statistics:_1989,
	address = {Chichester, England ; New York},
	series = {The {Manchester} physics series},
	title = {Statistics: {A} {Guide} to the {Use} of {Statistical} {Methods} in the {Physical} {Sciences}},
	isbn = {978-0-471-92294-0 978-0-471-92295-7},
	shorttitle = {Statistics},
	url = {https://www.wiley.com/en-us/Statistics%3A+A+Guide+to+the+Use+of+Statistical+Methods+in+the+Physical+Sciences-p-9780471922957},
	abstract = {The Manchester Physics Series General Editors: D. J. Sandiford; F.  Mandl; A. C. Phillips Department of Physics and Astronomy,  University of Manchester Properties of Matter B. H. Flowers and E.  Mendoza Optics Second Edition F. G. Smith and J. H. Thomson  Statistical Physics Second Edition F. Mandl Electromagnetism Second  Edition I. S. Grant and W. R. Phillips Statistics R. J. Barlow  Solid State Physics Second Edition J. R. Hook and H. E. Hall  Quantum Mechanics F. Mandl Particle Physics Second Edition B. R.  Martin and G. Shaw The Physics of Stars Second Edition A.C.  Phillips Computing for Scientists R. J. Barlow and A. R. Barnett  Written by a physicist, Statistics is tailored to the needs of  physical scientists, containing and explaining all they need to  know. It concentrates on parameter estimation, especially the  methods of Least Squares and Maximum Likelihood, but other  techniques, such as hypothesis testing, Bayesian statistics and  non-parametric methods are also included. Intended for reasonably  numerate scientists it contains all the basic formulae, their  derivations and applications, together with some more advanced  ones. Statistics features:  * Comprehensive coverage of the essential techniques physical  scientists are likely to need.  * A wealth of examples, and problems with their answers.  * Flexible structure and organisation allows it to be used as a  course text and a reference.  * A review of the basics, so that little prior knowledge is  required.},
	language = {en-us},
	note = {2018-06-05},
	publisher = {Wiley},
	author = {Barlow, Roger J.},
	year = {1989},
	keywords = {Statistical physics},
	annote = {Includes index},
	file = {Snapshot:/Users/mag/Zotero/storage/3KADWKVB/Statistics+A+Guide+to+the+Use+of+Statistical+Methods+in+the+Physical+Sciences-p-9780471922957.html:text/html}
}

@article{feroz_multinest:_2009,
	title = {{MultiNest}: an efficient and robust {Bayesian} inference tool for cosmology and particle physics},
	volume = {398},
	issn = {00358711, 13652966},
	shorttitle = {{MultiNest}},
	url = {http://arxiv.org/abs/0809.3437},
	doi = {10.1111/j.1365-2966.2009.14548.x},
	abstract = {We present further development and the first public release of our multimodal nested sampling algorithm, called MultiNest. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz \& Hobson (2008), which itself significantly outperformed existing MCMC techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MultiNest algorithm is demonstrated by application to two toy problems and to a cosmological inference problem focussing on the extension of the vanilla \${\textbackslash}Lambda\$CDM model to include spatial curvature and a varying equation of state for dark energy. The MultiNest software, which is fully parallelized using MPI and includes an interface to CosmoMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SuperBayeS package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org},
	number = {4},
	note = {2019-03-19},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Feroz, F. and Hobson, M. P. and Bridges, M.},
	month = oct,
	year = {2009},
	note = {arXiv: 0809.3437},
	keywords = {Astrophysics},
	pages = {1601--1614},
	annote = {Comment: 14 pages, 9 figures},
	file = {arXiv\:0809.3437 PDF:/Users/mag/Zotero/storage/98A4KBUF/Feroz et al. - 2009 - MultiNest an efficient and robust Bayesian infere.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/VFU3SN8T/0809.html:text/html}
}

@article{feroz_importance_2013,
	title = {Importance {Nested} {Sampling} and the {MultiNest} {Algorithm}},
	url = {http://arxiv.org/abs/1306.2144},
	abstract = {Bayesian inference involves two main computational challenges. First, in estimating the parameters of some model for the data, the posterior distribution may well be highly multi-modal: a regime in which the convergence to stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques becomes incredibly slow. Second, in selecting between a set of competing models the necessary estimation of the Bayesian evidence for each is, by definition, a (possibly high-dimensional) integration over the entire parameter space; again this can be a daunting computational task, although new Monte Carlo (MC) integration algorithms offer solutions of ever increasing efficiency. Nested sampling (NS) is one such contemporary MC strategy targeted at calculation of the Bayesian evidence, but which also enables posterior inference as a by-product, thereby allowing simultaneous parameter estimation and model selection. The widely-used MultiNest algorithm presents a particularly efficient implementation of the NS technique for multi-modal posteriors. In this paper we discuss importance nested sampling (INS), an alternative summation of the MultiNest draws, which can calculate the Bayesian evidence at up to an order of magnitude higher accuracy than `vanilla' NS with no change in the way MultiNest explores the parameter space. This is accomplished by treating as a (pseudo-)importance sample the totality of points collected by MultiNest, including those previously discarded under the constrained likelihood sampling of the NS algorithm. We apply this technique to several challenging test problems and compare the accuracy of Bayesian evidences obtained with INS against those from vanilla NS.},
	note = {2019-03-19},
	journal = {arXiv:1306.2144 [astro-ph, physics:physics, stat]},
	author = {Feroz, F. and Hobson, M. P. and Cameron, E. and Pettitt, A. N.},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.2144},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Data Analysis, Statistics and Probability, Statistics - Computation},
	annote = {Comment: 28 pages, 6 figures, 2 tables},
	file = {arXiv\:1306.2144 PDF:/Users/mag/Zotero/storage/MFR8PKFK/Feroz et al. - 2013 - Importance Nested Sampling and the MultiNest Algor.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/LNUNK4VY/1306.html:text/html}
}

@article{fryzlewicz_wild_2014,
	title = {Wild binary segmentation for multiple change-point detection},
	volume = {42},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1411.0858},
	doi = {10.1214/14-AOS1245},
	abstract = {We propose a new technique, called wild binary segmentation (WBS), for consistent estimation of the number and locations of multiple change-points in data. We assume that the number of change-points can increase to infinity with the sample size. Due to a certain random localisation mechanism, WBS works even for very short spacings between the change-points and/or very small jump magnitudes, unlike standard binary segmentation. On the other hand, despite its use of localisation, WBS does not require the choice of a window or span parameter, and does not lead to a significant increase in computational complexity. WBS is also easy to code. We propose two stopping criteria for WBS: one based on thresholding and the other based on what we term the `strengthened Schwarz information criterion'. We provide default recommended values of the parameters of the procedure and show that it offers very good practical performance in comparison with the state of the art. The WBS methodology is implemented in the R package wbs, available on CRAN. In addition, we provide a new proof of consistency of binary segmentation with improved rates of convergence, as well as a corresponding result for WBS.},
	language = {en},
	number = {6},
	note = {2019-03-28},
	journal = {The Annals of Statistics},
	author = {Fryzlewicz, Piotr},
	month = dec,
	year = {2014},
	note = {arXiv: 1411.0858},
	keywords = {Mathematics - Statistics Theory},
	pages = {2243--2281},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/14-AOS1245 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {Fryzlewicz - 2014 - Wild binary segmentation for multiple change-point.pdf:/Users/mag/Zotero/storage/Y6QN3MW5/Fryzlewicz - 2014 - Wild binary segmentation for multiple change-point.pdf:application/pdf}
}

@misc{noauthor_occams_nodate,
	title = {Occam's razor - {Wikiwand}},
	url = {https://www.wikiwand.com/en/Occam%27s_razor},
	note = {2019-04-01},
	file = {Occam's razor - Wikiwand:/Users/mag/Zotero/storage/EQBVNHBN/Occam's_razor.html:text/html}
}

@article{courtney_comments_2008,
	title = {Comments {Regarding} "{On} the {Nature} of {Science}"},
	url = {http://arxiv.org/abs/0812.4932},
	abstract = {An attempt to redefine science in the 21st century (BK Jennings, On the Nature of Science, Physics in Canada, 63(7) 2007) has abandoned traditional notions of natural law and objective reality, blurred the distinctions between natural science and natural history, elevated Occam's razor from an epistemological preference to a scientific principle, and elevated peer-review and experimental care as equals with repeatable experiment as arbiters of scientific validity. Our comments review the long-established axioms of the scientific method, remind readers of the distinctions between science and history, disprove the generality of Occam's razor by counter example, and highlight the risks of accepting additional scientific arbiters as equal to repeatable experiment.},
	note = {2019-04-02},
	journal = {arXiv:0812.4932 [physics]},
	author = {Courtney, Amy and Courtney, Michael},
	month = dec,
	year = {2008},
	note = {arXiv: 0812.4932},
	keywords = {Physics - History and Philosophy of Physics},
	file = {arXiv\:0812.4932 PDF:/Users/mag/Zotero/storage/XBWG8CBQ/Courtney and Courtney - 2008 - Comments Regarding On the Nature of Science.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/SD4WV5QY/0812.html:text/html}
}

@book{bishop_pattern_2009,
	address = {New York, NY},
	edition = {Corrected at 8th printing 2009},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2 978-1-4939-3843-8},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2009},
	note = {OCLC: 845772798},
	file = {Bishop - 2009 - Pattern recognition and machine learning.pdf:/Users/mag/Zotero/storage/KLJE7ZZ2/Bishop - 2009 - Pattern recognition and machine learning.pdf:application/pdf}
}

@book{hastie_elements_2017,
	address = {New York, NY},
	edition = {Second edition, corrected at 12th printing 2017},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	language = {en},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
	year = {2017},
	note = {OCLC: 995842694},
	file = {Hastie et al. - 2017 - The elements of statistical learning data mining,.pdf:/Users/mag/Zotero/storage/GAVINNLK/Hastie et al. - 2017 - The elements of statistical learning data mining,.pdf:application/pdf}
}

@book{sterling_1001_2013,
	address = {Hoboken, New Jersey},
	series = {--{For} dummies},
	title = {1001 algebra {II} practice problems for dummies},
	isbn = {978-1-118-44662-1},
	abstract = {Practice solving the problems that you'll encounter in your Algebra II course. This guide starts with a review of Algebra basics and ends with sequences, sets, and counting techniques. Each practice question includes not only an answer but a step-by-step explanation --},
	publisher = {For Dummies, a Wiley Brand},
	author = {Sterling, Mary Jane},
	year = {2013},
	note = {OCLC: ocn828416495},
	keywords = {Algebra, Problems, exercises, etc},
	annote = {Includes index},
	annote = {Reviewing Algebra basics -- Solving quadratic equations and nonlinear inequalities -- Solving radical and rational equations -- Graphs and equations of lines -- Functions -- Quadratic functions and relations -- Polynomial functions and equations -- Rational functions -- Exponential and logarithmic functions -- Conic sections -- Systems of linear equations -- Systems of nonlinear equations and inequalities -- Working with complex numbers -- Matrices -- Sequences and series -- Sets -- Counting techniques and probability -- The answers}
}

@misc{paisley_columbiax:_2018,
	address = {Colombia University},
	type = {Lecture},
	title = {{ColumbiaX}: {Machine} {Learning}},
	author = {Paisley, John},
	month = jan,
	year = {2018}
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	note = {2019-04-02},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	month = oct,
	year = {2011},
	pages = {2825−2830},
	file = {Fulltext PDF:/Users/mag/Zotero/storage/7PIRJVDC/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf}
}

@article{efron_least_2004,
	title = {Least angle regression},
	volume = {32},
	issn = {0090-5364},
	abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C-p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
	language = {English},
	number = {2},
	journal = {Annals of Statistics},
	author = {Efron, B. and Hastie, T. and Johnstone, I. and Tibshirani, R.},
	month = apr,
	year = {2004},
	note = {WOS:000221411000001},
	keywords = {boosting, coefficient paths, lasso, linear regression, selection, variable   selection},
	pages = {407--451},
	file = {LeastAngle_2002.pdf:/Users/mag/Zotero/storage/T7F2ZVHI/LeastAngle_2002.pdf:application/pdf}
}

@misc{noauthor_karakterfordeling_nodate,
	title = {Karakterfordeling},
	url = {http://karakterstatistik.stads.ku.dk/},
	note = {2019-03-27},
	file = {Karakterfordeling:/Users/mag/Zotero/storage/Y88USUJK/karakterstatistik.stads.ku.dk.html:text/html}
}

@misc{noauthor_selenium_nodate,
	title = {Selenium {Client} {Driver} — {Selenium} 3.14 documentation},
	url = {https://seleniumhq.github.io/selenium/docs/api/py/index.html},
	note = {2019-04-02},
	file = {Selenium Client Driver — Selenium 3.14 documentation:/Users/mag/Zotero/storage/WFB6BJG4/index.html:text/html}
}

@misc{noauthor_scrapy_nodate,
	title = {Scrapy 1.6 documentation — {Scrapy} 1.6.0 documentation},
	url = {https://docs.scrapy.org/en/latest/},
	note = {2019-04-02},
	file = {Scrapy 1.6 documentation — Scrapy 1.6.0 documentation:/Users/mag/Zotero/storage/G6LN4YB2/latest.html:text/html}
}

@article{mckinney_data_2010,
	title = {Data {Structures} for {Statistical} {Computing} in {Python}},
	abstract = {In this paper we are concerned with the practical issues of working with data sets common to ﬁnance, statistics, and other related ﬁelds. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss speciﬁc design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
	language = {en},
	author = {McKinney, Wes},
	year = {2010},
	pages = {6},
	file = {McKinney - 2010 - Data Structures for Statistical Computing in Pytho.pdf:/Users/mag/Zotero/storage/UQAYVWI7/McKinney - 2010 - Data Structures for Statistical Computing in Pytho.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	abstract = {We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {English},
	number = {1},
	journal = {Journal of the Royal Statistical Society Series B-Methodological},
	author = {Tibshirani, R.},
	year = {1996},
	note = {WOS:A1996TU31400017},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288},
	file = {lasso.pdf:/Users/mag/Zotero/storage/I8DZZKPM/lasso.pdf:application/pdf}
}
